{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, AutoProcessor, WavLMModel, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import yaml\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_class_by_name(module_name, class_name):\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, class_name)\n",
    "\n",
    "\n",
    "\n",
    "get_class_by_name('transformers', 'Wav2Vec2Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(file_path: str) -> dict[dict[str, str]]:\n",
    "    with open(file_path, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "YAML_PATH = 'params.yaml'\n",
    "VARS = load_yaml(YAML_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "for i, j in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'microsoft/wavlm-large'\n",
    "PROCESSOR = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "MODEL = WavLMModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'processor': 'Wav2Vec2FeatureExtractor',\n",
       "  'model': 'Wav2Vec2Model',\n",
       "  'name': 'jonatasgrosman/wav2vec2-large-xlsr-53-english'},\n",
       " {'processor': 'Wav2Vec2FeatureExtractor',\n",
       "  'model': 'WavLMModel',\n",
       "  'name': 'microsoft/wavlm-large'},\n",
       " {'processor': 'Wav2Vec2FeatureExtractor',\n",
       "  'model': 'HubertModel',\n",
       "  'name': 'facebook/hubert-large-ls960-ft'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VARS['audio_models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_single_audio_embedding(file_path: str) -> torch.tensor:\n",
    "    waveform, sample_rate = librosa.load(file_path, sr = 16000)\n",
    "    file_emo_alias = file_path.split('/')[-1].split('_')[2]\n",
    "    label = VARS['emotion_mapping'][file_emo_alias]\n",
    "    inputs = PROCESSOR(waveform, sampling_rate = sample_rate, return_tensors = 'pt', padding = True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = MODEL(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "    global_embedding = torch.mean(last_hidden_state, dim = 1)\n",
    "    \n",
    "    return global_embedding.squeeze(0), torch.tensor(label)\n",
    "\n",
    "\n",
    "def get_all_audio_embeddings(root: str) -> dict[torch.tensor, torch.tensor]:\n",
    "    embeddings = dict()\n",
    "    paths = sorted([os.path.join(root, file) for file in os.listdir(root) if (file.endswith('.wav') and file != '1076_MTI_SAD_XX.wav')])\n",
    "    \n",
    "    for path in paths:\n",
    "        embedding, label = get_single_audio_embedding(path)\n",
    "        embeddings[embedding] = label\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1441,  0.1018, -0.0264,  ...,  0.0402, -0.0409, -0.1608])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = get_single_audio_embedding('data/audio_data/1001_DFA_ANG_XX.wav')\n",
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WavLM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "best_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
