{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import openl3\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Studia\\.conda\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "audio_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = os.listdir('data/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7442/7442 [16:44<00:00,  7.41it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_embeddings = []\n",
    "for file in tqdm(audio_files):\n",
    "    audio, sr = librosa.load(f'data/audio/{file}', sr=16000)\n",
    "    inputs = audio_processor(audio, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = audio_model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    audio_embeddings.append(embedding[0])\n",
    "# audio_embeddings = [embedding[0] for embedding in audio_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"audio_embeddings.npy\", np.array(audio_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_embeddings = np.load(\"audio_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7442, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, num_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_size (int): Rozmiar wektora wejściowego (np. 768).\n",
    "        - layer_sizes (list of int): Lista z rozmiarami każdej warstwy ukrytej.\n",
    "        - num_classes (int): Liczba klas w klasyfikacji.\n",
    "        \"\"\"\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        in_features = input_size\n",
    "        for out_features in layer_sizes:\n",
    "            layers.append(nn.Linear(in_features, out_features))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_features = out_features\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    \"\"\"\n",
    "    Funkcja do trenowania modelu.\n",
    "    \n",
    "    Args:\n",
    "    - model: Model PyTorch do trenowania.\n",
    "    - train_loader: Dataloader dla danych treningowych.\n",
    "    - val_loader: Dataloader dla danych walidacyjnych.\n",
    "    - criterion: Funkcja kosztu (np. CrossEntropyLoss).\n",
    "    - optimizer: Optymalizator (np. Adam).\n",
    "    - epochs: Liczba epok.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_targets = []\n",
    "        train_preds = []\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        train_accuracy = accuracy_score(train_targets, train_preds)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_targets = []\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 768               # Rozmiar wejścia (embedding)\n",
    "layer_sizes = [512, 256, 128]  # Rozmiary warstw ukrytych\n",
    "num_classes = 6                # Liczba klas\n",
    "\n",
    "model = MLPClassifier(input_size, layer_sizes, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('data/audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = []\n",
    "y = []\n",
    "for file in files:\n",
    "    emo = file.split('_')[2]\n",
    "    sp = file.split('_')[0]\n",
    "    y.append(emo)\n",
    "    speaker.append(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5721"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(c.values())[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1065"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(c.values())[70:83])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list(c.values())[83:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_map = {\n",
    "    'NEU': 0,\n",
    "    'HAP': 1,\n",
    "    'SAD': 2,\n",
    "    'ANG': 3,\n",
    "    'FEA': 4,\n",
    "    'DIS': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.Tensor(audio_embeddings[:5721])\n",
    "x_val = torch.Tensor(audio_embeddings[5721:6786])\n",
    "x_test = torch.Tensor(audio_embeddings[6786:])\n",
    "y_train = torch.Tensor([y_map[a] for a in y[:5721]]).long()\n",
    "y_val = torch.Tensor([y_map[a] for a in y[5721:6786]]).long()\n",
    "y_test = torch.Tensor([y_map[a] for a in y[6786:]]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.4534, Train Accuracy: 0.4036\n",
      "Val Loss: 1.5624, Val Accuracy: 0.4254\n",
      "--------------------------------------------------\n",
      "Epoch 2/20\n",
      "Train Loss: 1.2893, Train Accuracy: 0.4837\n",
      "Val Loss: 1.3892, Val Accuracy: 0.4695\n",
      "--------------------------------------------------\n",
      "Epoch 3/20\n",
      "Train Loss: 1.2099, Train Accuracy: 0.5268\n",
      "Val Loss: 1.6170, Val Accuracy: 0.4423\n",
      "--------------------------------------------------\n",
      "Epoch 4/20\n",
      "Train Loss: 1.1649, Train Accuracy: 0.5508\n",
      "Val Loss: 1.5867, Val Accuracy: 0.4479\n",
      "--------------------------------------------------\n",
      "Epoch 5/20\n",
      "Train Loss: 1.1128, Train Accuracy: 0.5770\n",
      "Val Loss: 1.5235, Val Accuracy: 0.4498\n",
      "--------------------------------------------------\n",
      "Epoch 6/20\n",
      "Train Loss: 1.0820, Train Accuracy: 0.5903\n",
      "Val Loss: 1.5010, Val Accuracy: 0.4948\n",
      "--------------------------------------------------\n",
      "Epoch 7/20\n",
      "Train Loss: 1.0556, Train Accuracy: 0.5966\n",
      "Val Loss: 1.4963, Val Accuracy: 0.4657\n",
      "--------------------------------------------------\n",
      "Epoch 8/20\n",
      "Train Loss: 1.0342, Train Accuracy: 0.6011\n",
      "Val Loss: 1.2766, Val Accuracy: 0.5408\n",
      "--------------------------------------------------\n",
      "Epoch 9/20\n",
      "Train Loss: 0.9944, Train Accuracy: 0.6282\n",
      "Val Loss: 1.3530, Val Accuracy: 0.5474\n",
      "--------------------------------------------------\n",
      "Epoch 10/20\n",
      "Train Loss: 0.9872, Train Accuracy: 0.6219\n",
      "Val Loss: 1.3794, Val Accuracy: 0.5437\n",
      "--------------------------------------------------\n",
      "Epoch 11/20\n",
      "Train Loss: 0.9635, Train Accuracy: 0.6326\n",
      "Val Loss: 1.4080, Val Accuracy: 0.5380\n",
      "--------------------------------------------------\n",
      "Epoch 12/20\n",
      "Train Loss: 0.9416, Train Accuracy: 0.6434\n",
      "Val Loss: 1.3022, Val Accuracy: 0.5352\n",
      "--------------------------------------------------\n",
      "Epoch 13/20\n",
      "Train Loss: 0.9386, Train Accuracy: 0.6488\n",
      "Val Loss: 1.3844, Val Accuracy: 0.5446\n",
      "--------------------------------------------------\n",
      "Epoch 14/20\n",
      "Train Loss: 0.9257, Train Accuracy: 0.6494\n",
      "Val Loss: 1.5416, Val Accuracy: 0.5042\n",
      "--------------------------------------------------\n",
      "Epoch 15/20\n",
      "Train Loss: 0.8996, Train Accuracy: 0.6593\n",
      "Val Loss: 1.2789, Val Accuracy: 0.5559\n",
      "--------------------------------------------------\n",
      "Epoch 16/20\n",
      "Train Loss: 0.8803, Train Accuracy: 0.6682\n",
      "Val Loss: 1.5076, Val Accuracy: 0.5230\n",
      "--------------------------------------------------\n",
      "Epoch 17/20\n",
      "Train Loss: 0.8643, Train Accuracy: 0.6768\n",
      "Val Loss: 1.4996, Val Accuracy: 0.5399\n",
      "--------------------------------------------------\n",
      "Epoch 18/20\n",
      "Train Loss: 0.8558, Train Accuracy: 0.6756\n",
      "Val Loss: 1.5297, Val Accuracy: 0.5512\n",
      "--------------------------------------------------\n",
      "Epoch 19/20\n",
      "Train Loss: 0.8361, Train Accuracy: 0.6822\n",
      "Val Loss: 1.3593, Val Accuracy: 0.5690\n",
      "--------------------------------------------------\n",
      "Epoch 20/20\n",
      "Train Loss: 0.8382, Train Accuracy: 0.6796\n",
      "Val Loss: 1.4908, Val Accuracy: 0.5559\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
